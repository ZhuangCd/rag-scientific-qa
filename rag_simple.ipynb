{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41125219",
   "metadata": {},
   "source": [
    "# Simple RAG for Scientific Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f5eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# print(\"TF:\", tf.__version__)\n",
    "# print(\"Devices:\", tf.config.list_physical_devices())\n",
    "# print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d2ced6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uv pip install chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f14054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 papers\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load scientific papers from JSON ---\n",
    "import json\n",
    "import os\n",
    "\n",
    "papers_dir = \"../papers_json_3\"\n",
    "\n",
    "corpus = []\n",
    "files = sorted([f for f in os.listdir(papers_dir) if f.endswith('.json')])[:200]\n",
    "\n",
    "for filename in files:\n",
    "    with open(os.path.join(papers_dir, filename), 'r', encoding='utf-8') as f:\n",
    "        paper = json.load(f)\n",
    "    corpus.append({\n",
    "        \"article_id\": paper.get(\"article_id\", filename.replace(\".json\", \"\")),\n",
    "        \"text\": paper.get(\"abstract\", \"\") + \"\\n\\n\" + paper.get(\"article\", \"\")\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(corpus)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d837275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 13141\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Overlapping chunking (window 150, overlap 50, only if > 100 tokens) ---\n",
    "'''\n",
    "chunk size in this case refers to number of words (tokens)\n",
    "'''\n",
    "def chunk_text(text, chunk_size=150, overlap=50):\n",
    "    words = text.split()\n",
    "    if len(words) <= 100:\n",
    "        return [text] if words else []\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Build chunks with metadata\n",
    "chunk_texts, metadatas, ids = [], [], []\n",
    "\n",
    "for paper in corpus:\n",
    "    chunks = chunk_text(paper[\"text\"])\n",
    "    for idx, ch in enumerate(chunks):\n",
    "        chunk_texts.append(ch)\n",
    "        metadatas.append({\"article_id\": paper[\"article_id\"], \"chunk_idx\": idx})\n",
    "        ids.append(f'{paper[\"article_id\"]}_chunk_{idx}')\n",
    "\n",
    "print(f\"Total chunks: {len(chunk_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c9fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Initialize ChromaDB ---\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"scientific_rag_db\")\n",
    "collection = client.get_or_create_collection(\"scientific_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2be9fccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already has 13141 chunks\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Embed and index chunks ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Using smaller, faster SBERT model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "if collection.count() == 0:\n",
    "    print(f\"Embedding and indexing {len(chunk_texts)} chunks...\")\n",
    "    \n",
    "    # Embed and add in batches (ChromaDB has max batch size ~5000)\n",
    "    batch_size = 500\n",
    "    for i in tqdm(range(0, len(chunk_texts), batch_size), desc=\"Indexing\"):\n",
    "        batch_texts = chunk_texts[i:i + batch_size]\n",
    "        batch_metas = metadatas[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        \n",
    "        batch_embs = embedder.encode(batch_texts, show_progress_bar=False).tolist()\n",
    "        \n",
    "        collection.add(\n",
    "            documents=batch_texts,\n",
    "            embeddings=batch_embs,\n",
    "            metadatas=batch_metas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "    \n",
    "    print(f\"Indexed {collection.count()} chunks\")\n",
    "else:\n",
    "    print(f\"Collection already has {collection.count()} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b2bf673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] article_1 (chunk 6)\n",
      "    occupation probability , and coverage , have tight relationships with the structure of the graph upon which the walk takes place @xcite . for this rea...\n",
      "\n",
      "[2] article_1 (chunk 9)\n",
      "    walkers on the topological properties of the nodes at each layer in order to perform an efficient exploration of such systems . we notice that random ...\n",
      "\n",
      "[3] article_1 (chunk 8)\n",
      "    communities @xcite , and provide optimal exploration of a network using only local information @xcite . it has also been found that the dynamics of de...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Retrieval function ---\n",
    "def retrieve(query, k=3):\n",
    "    q_emb = embedder.encode([query]).tolist()[0]\n",
    "    results = collection.query(query_embeddings=[q_emb], n_results=k)\n",
    "    return results[\"documents\"][0], results[\"metadatas\"][0]\n",
    "\n",
    "# Test retrieval\n",
    "docs, metas = retrieve(\"random walk on networks\")\n",
    "for i, (doc, meta) in enumerate(zip(docs, metas)):\n",
    "    print(f\"[{i+1}] {meta['article_id']} (chunk {meta['chunk_idx']})\")\n",
    "    print(f\"    {doc[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9570c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model loaded: Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Load LLM ---\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available, otherwise CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "print(f\"Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8e40c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. RAG answer generation ---\n",
    "def rag_answer(query):\n",
    "    docs, metas = retrieve(query)\n",
    "    \n",
    "    # Build context with source references\n",
    "    context_parts = []\n",
    "    for i, (doc, meta) in enumerate(zip(docs, metas), 1):\n",
    "        context_parts.append(f\"[{i}] Source: {meta['article_id']}\\n{doc}\")\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    prompt = f\"\"\"Use the following context to answer the question. Cite sources using [1], [2], etc.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**tokens, max_new_tokens=200, do_sample=False)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the answer part\n",
    "    if \"ANSWER:\" in answer:\n",
    "        answer = answer.split(\"ANSWER:\")[-1].strip()\n",
    "    \n",
    "    # Return answer and sources\n",
    "    sources = [meta['article_id'] for meta in metas]\n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79a65dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Where is the jet energy dissipated relative to the cool core in the system discussed?\n",
      "\n",
      "ANSWER: The jet energy is dissipated outside the cool core, according to the information provided in the text.\n",
      "Sources:\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "\n",
      "SOURCES: article_4, article_4, article_4\n",
      "============================================================\n",
      "QUESTION: What determines the actual sensitivity of the observational setup mentioned?\n",
      "\n",
      "ANSWER: The actual sensitivity of the observational setup, as determined by the source material provided, depends on several factors including the type of experiment being conducted, the specific instruments used, and the theoretical models employed to predict the observable signals. The text mentions that \"a near detector is also considered in the analysis,\" suggesting that this additional component could provide valuable information about the system's performance. Furthermore, it notes that \"the assumed true values of the oscillation parameters are @xmath129 in the analysis,\" indicating that these parameters play a crucial role in determining the sensitivity based on their known values. However, without more detailed information about the specific experiment and its underlying physics, it's difficult to pinpoint exactly what determines the sensitivity. Sources [1], [2], and [3] do not directly address the sensitivity issue but instead discuss various aspects such as the use of near detectors and the introduction of systematic uncertainties. Therefore, while there may be multiple determinants, the exact mechanism remains unclear from the given context\n",
      "\n",
      "SOURCES: article_56, article_166, article_56\n",
      "============================================================\n",
      "QUESTION: What are the two possible mechanisms by which photons may be produced in the described process?\n",
      "\n",
      "ANSWER: The two possible mechanisms by which photons may be produced in the described process are through the fragmentation of quarks or gluons, and through the direct production of photons without any initial state radiation.\n",
      "Sources:\n",
      "[1] Source: article_34\n",
      "[2] Source: article_118\n",
      "[3] Source: article_34\n",
      "\n",
      "SOURCES: article_34, article_118, article_34\n",
      "============================================================\n",
      "QUESTION: How are sources and destinations paired in the network model?\n",
      "\n",
      "ANSWER: An aggregate throughput is achieved with high probability, according to Theorem 1.\n",
      "Sources and destinations are paired in the network model based on the conditions stated in Theorem 1, where \"with high probability\" indicates that the desired outcome occurs frequently within the network system under consideration. This theorem establishes the possibility of achieving a certain level of performance or efficiency (in this case, an aggregate throughput) across various combinations of source-destination pairings. The theorem's application implies that these pairings are strategically designed to optimize communication or data flow through the network, ensuring that resources are utilized effectively without significant loss or inefficiency. Sources and destinations are thus connected in a way that maximizes their potential impact on each other, facilitating smooth interaction and information exchange throughout the network.\n",
      "\n",
      "SOURCES: article_103, article_103, article_1\n",
      "============================================================\n",
      "QUESTION: What conclusion is reached about the consistency of the constraint algebra for quantum Bohmian trajectories?\n",
      "\n",
      "ANSWER: The conclusion reached about the consistency of the constraint algebra for quantum Bohmian trajectories is that, when restricted to the Bohmian trajectories, the resulting Hamiltonian evolution is consistent only if the corresponding momentum term (@xmath266) is weakly zero due to the Bohm's guidance relations. This implies that the quantum superhamiltonian must be evaluated within the framework of Bohmian mechanics to ensure consistency across all possible trajectories. The key point here is that the algebraic closure of the constraints is crucial for maintaining consistency in the quantum superhamiltonian formulation.\n",
      "\n",
      "SOURCES: article_198, article_198, article_198\n",
      "============================================================\n",
      "QUESTION: On what observational sample size and survey are the presented galaxy results based?\n",
      "\n",
      "ANSWER: The presented galaxy results are based on an ACS (Advanced Camera for Surveys) survey.\n",
      "Sources:\n",
      "[1] Source: article_173\n",
      "[2] Source: article_122\n",
      "[3] Source: article_173\n",
      "\n",
      "SOURCES: article_173, article_122, article_173\n",
      "============================================================\n",
      "QUESTION: Which theoretical framework motivates the idea that the universe contains many light scalar fields?\n",
      "\n",
      "ANSWER: Theoretical frameworks motivated by landscape models suggest that our universe contains a vast number of light scalar fields, which are expected to arise from the presence of multiple scalar fields in theories of high-energy physics beyond the standard model. This concept is supported by the idea that the universe may contain a multitude of scalar fields due to the existence of multiple scalar fields within the landscape of possible universes. Landscape models provide a way to understand how different regions of the parameter space of the effective field theory might correspond to distinct physical realities, including those that could lead to the observed properties of the universe. Sources 1 and 2 support this explanation.\n",
      "\n",
      "SOURCES: article_23, article_23, article_23\n",
      "============================================================\n",
      "QUESTION: Why is the mentioned gamma-ray binary system considered particularly interesting?\n",
      "\n",
      "ANSWER: The mentioned gamma-ray binary system, PSR B1259-63/LS 2883, is considered particularly interesting due to its unique property of having the nature of the compact object known. This makes it one of the few systems where the identity of the neutron star or black hole can be determined through observational means. Additionally, the system exhibits significant gamma-ray emission patterns that provide insights into the physics occurring within the binary environment, such as the interaction between the pulsar's magnetic field and the stellar wind, leading to the formation of a stand-off shock front. The presence of an associated circumstellar disc further complicates the understanding of the system, as the disc contributes significantly to the observed gamma-ray emissions, especially at periastron when the disc is most illuminated by the central object. These factors collectively make the system a valuable subject for astrophysical research, offering opportunities to study various aspects of relativistic phenomena and accretion processes in extreme environments. [1\n",
      "\n",
      "SOURCES: article_99, article_99, article_99\n",
      "============================================================\n",
      "QUESTION: What key assumption about turbulence underlies the basis of the present work on local equilibrium patches?\n",
      "\n",
      "ANSWER: The key assumption underlying the basis of the present work on local equilibrium patches is that local equilibrium patches are embedded within turbulent systems, forming the foundation of the research. This perspective suggests that understanding the relationship between local equilibrium structures and turbulent dynamics could provide insights into broader aspects of magnetohydrodynamics (MHD) and its applications in astrophysical contexts like solar wind turbulence. The study aims to explore how local AlfvÃ©nic states correlate with Taylor equilibria, aiming to uncover correlations and relationships that might not be apparent at larger scales or over longer timescales. Sources [1], [2], and [3] support this interpretation, highlighting the focus on investigating the interplay between local equilibrium structures and turbulent dynamics.\n",
      "\n",
      "SOURCES: article_154, article_154, article_154\n",
      "============================================================\n",
      "QUESTION: How does the mass-ratio distribution of stellar companions to hot Jupiter systems compare to that of field star binaries?\n",
      "\n",
      "ANSWER: The mass-ratio distribution for binaries hosting hot Jupiters peaks at small mass ratios, unlike the relatively uniform distribution of mass ratios observed for field star binaries. Although this may in part reflect a bias against equal mass binaries in photometric transit surveys, it is also plausible that higher mass companions might actively suppress planet formation by disrupting the protoplanetary disk. As discussed in section [sec: moregiants], the more subtle effects of a low-mass companion on the disk structure could instead aid in planet formation by creating regions of locally enhanced density. Alternatively, protoplanetary disk masses in binary star systems may be higher than those of their isolated counterparts, resulting in globally enhanced disk densities.\n",
      "Sources:\n",
      "[1]\n",
      "[2]\n",
      "[3]\n",
      "\n",
      "SOURCES: article_55, article_55, article_55\n",
      "============================================================\n",
      "QUESTION: Which numerical model is employed for microswimmers, and how are the swimmers represented in this model?\n",
      "\n",
      "ANSWER: The numerical model for microswimmers is the squirmer model, which employs rigid particles with a prescribed flow field on their surface. The squirmer model is expressed in the form of an infinite expansion, with components in the tangential, radial, and azimuthal directions. However, utilizing only the first two modes of the tangential field, following equation ([eq: sq_2]), already enables the modeling of different types of swimmers, including pushers, pullers, and neutral swimmers. In this model, the prescribed flow field is assumed to be an axisymmetric pure tangential one and the particle is rigid and spherical. This model captures the features of artificial microswimmers such as Janus particles, although it was originally intended to describe ciliary propulsion of microorganisms. To solve the combined fluid-solid problem, the smoothed profile method is employed. Radial, and azimuthal directions are utilized, but only the first two modes of the tangential field are considered. The coefficient of\n",
      "\n",
      "SOURCES: article_9, article_9, article_9\n",
      "============================================================\n",
      "QUESTION: Why is the magnetic moment considered a fundamental property of the nucleon?\n",
      "\n",
      "ANSWER: The magnetic moment of the nucleon is considered a fundamental property because it plays a crucial role in understanding the behavior of particles within the nucleus and their interactions with external fields such as electric or magnetic fields. The magnetic moment arises due to the intrinsic properties of the nucleons themselves, specifically their charge and spin, which are inherent characteristics of the subatomic particles making up the nucleon. This intrinsic nature makes the magnetic moment a fundamental aspect of particle physics, essential for describing how nucleons interact with other particles and contribute to the overall dynamics of atomic nuclei and larger systems. Sources 1 and 2 provide detailed explanations of how the magnetic moment is calculated and understood within the framework of quantum chromodynamics (QCD), emphasizing its importance in both theoretical models and experimental measurements.\n",
      "\n",
      "SOURCES: article_98, article_98, article_98\n",
      "============================================================\n",
      "QUESTION: Under what restriction does the algebra of constraints remain closed, avoiding inconsistency in quantum geometrodynamics and quantum field theory?\n",
      "\n",
      "ANSWER: The algebra of constraints remains closed under the Dirac-Teitelboim algebra, ensuring consistency in both quantum geometrodynamics and quantum field theory.\n",
      "Sources:\n",
      "[1]\n",
      "[2]\n",
      "\n",
      "SOURCES: article_198, article_198, article_198\n",
      "============================================================\n",
      "QUESTION: What do clusters of galaxies trace in the context of large-scale cosmic structure?\n",
      "\n",
      "ANSWER: Clusters of galaxies trace the largest gravitationally bound mass concentrations in the universe.\n",
      "This information is sourced from [1]. The text states, \"clusters of galaxies trace the largest gravitationally bound mass concentrations in the universe.\" This directly answers the question asked.\n",
      "\n",
      "SOURCES: article_120, article_65, article_65\n",
      "============================================================\n",
      "QUESTION: Why are radiative decays of heavy quarkonia useful for studying color-singlet two-gluon systems?\n",
      "\n",
      "ANSWER: Radiative decays of heavy quarkonia are useful for studying color-singlet two-gluon systems because they allow researchers to analyze the behavior of two gluons, which are fundamental particles involved in the strong interaction between quarks. By observing these decays, scientists can gain insights into how gluons combine to form mesons and other bound states, which are crucial for understanding the dynamics of QCD (Quantum Chromodynamics), the theory governing the strong force between quarks. This research helps in refining theoretical models like non-relativistic QCD and soft-collinear effective theory, as well as improving calculations involving gluon distribution amplitudes and perturbative QCD. Additionally, it provides valuable constraints on the production of glueballs, which are hypothetical particles made up entirely of gluons. The use of such decays also allows for precise measurement of branching ratios and helicity production ratios, contributing significantly to the field's understanding of high-energy physics phenomena. Sources [1]\n",
      "\n",
      "SOURCES: article_31, article_31, article_31\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Test queries ---\n",
    "test_queries = [\n",
    "\"Where is the jet energy dissipated relative to the cool core in the system discussed?\",\n",
    "\"What determines the actual sensitivity of the observational setup mentioned?\",\n",
    "\"What are the two possible mechanisms by which photons may be produced in the described process?\",\n",
    "\"How are sources and destinations paired in the network model?\",\n",
    "\"What conclusion is reached about the consistency of the constraint algebra for quantum Bohmian trajectories?\",\n",
    "\"On what observational sample size and survey are the presented galaxy results based?\",\n",
    "\"Which theoretical framework motivates the idea that the universe contains many light scalar fields?\",\n",
    "\"Why is the mentioned gamma-ray binary system considered particularly interesting?\",\n",
    "\"What key assumption about turbulence underlies the basis of the present work on local equilibrium patches?\",\n",
    "\"How does the mass-ratio distribution of stellar companions to hot Jupiter systems compare to that of field star binaries?\",\n",
    "\"Which numerical model is employed for microswimmers, and how are the swimmers represented in this model?\",\n",
    "\"Why is the magnetic moment considered a fundamental property of the nucleon?\",\n",
    "\"Under what restriction does the algebra of constraints remain closed, avoiding inconsistency in quantum geometrodynamics and quantum field theory?\",\n",
    "\"What do clusters of galaxies trace in the context of large-scale cosmic structure?\",\n",
    "\"Why are radiative decays of heavy quarkonia useful for studying color-singlet two-gluon systems?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"QUESTION: {q}\")\n",
    "    answer, sources = rag_answer(q)\n",
    "    print(f\"\\nANSWER: {answer.split('ANSWER:')[-1].strip()}\")\n",
    "    print(f\"\\nSOURCES: {', '.join(sources)}\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-scientific-qa (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
