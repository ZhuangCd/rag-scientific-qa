{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41125219",
   "metadata": {},
   "source": [
    "# Simple RAG for Scientific Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ced6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.5 environment at: C:\\Users\\balog\\AppData\\Local\\Programs\\Python\\Python312\u001b[0m\n",
      "  \u001b[31m×\u001b[0m Failed to read `websocket-client==1.8.0`\n",
      "\u001b[31m  ├─▶ \u001b[0mFailed to read metadata from installed package `websocket-client==1.8.0`\n",
      "\u001b[31m  ╰─▶ \u001b[0mfailed to open file\n",
      "\u001b[31m      \u001b[0m`C:\\Users\\balog\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\websocket_client-1.8.0.dist-info\\METADATA`:\n",
      "\u001b[31m      \u001b[0mThe system cannot find the file specified. (os error 2)\n",
      "\u001b[36m  help: \u001b[0m`\u001b[36mwebsocket-client\u001b[39m` (\u001b[36mv1.8.0\u001b[39m) was included because `\u001b[36mchromadb\u001b[39m`\n",
      "        (\u001b[36mv1.4.1\u001b[39m) depends on `\u001b[36mkubernetes\u001b[39m\u001b[36m>=28.1.0\u001b[39m` (\u001b[36mv35.0.0\u001b[39m) which depends\n",
      "        on `\u001b[36mwebsocket-client\u001b[39m\u001b[36m>=0.32.0, <0.40.0 | >0.40.0, <0.41.dev0 |\n",
      "        >=0.43.dev0\u001b[39m`\n"
     ]
    }
   ],
   "source": [
    "#uv pip install chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f14054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 papers\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load scientific papers from JSON ---\n",
    "import json\n",
    "import os\n",
    "\n",
    "papers_dir = \"../papers_json_3/papers_json_3\"\n",
    "\n",
    "corpus = []\n",
    "files = sorted([f for f in os.listdir(papers_dir) if f.endswith('.json')])[:200]\n",
    "\n",
    "for filename in files:\n",
    "    with open(os.path.join(papers_dir, filename), 'r', encoding='utf-8') as f:\n",
    "        paper = json.load(f)\n",
    "    corpus.append({\n",
    "        \"article_id\": paper.get(\"article_id\", filename.replace(\".json\", \"\")),\n",
    "        \"text\": paper.get(\"abstract\", \"\") + \"\\n\\n\" + paper.get(\"article\", \"\")\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(corpus)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 13141\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Overlapping chunking (window 150, overlap 50, only if > 100 tokens) ---\n",
    "\n",
    "def chunk_text(text, chunk_size=150, overlap=50):\n",
    "    words = text.split()\n",
    "    if len(words) <= 100:\n",
    "        return [text] if words else []\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Build chunks with metadata\n",
    "chunk_texts, metadatas, ids = [], [], []\n",
    "\n",
    "for paper in corpus:\n",
    "    chunks = chunk_text(paper[\"text\"])\n",
    "    for idx, ch in enumerate(chunks):\n",
    "        chunk_texts.append(ch)\n",
    "        metadatas.append({\"article_id\": paper[\"article_id\"], \"chunk_idx\": idx})\n",
    "        ids.append(f'{paper[\"article_id\"]}_chunk_{idx}')\n",
    "\n",
    "print(f\"Total chunks: {len(chunk_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c9fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Initialize ChromaDB ---\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"scientific_rag_db\")\n",
    "collection = client.get_or_create_collection(\"scientific_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be9fccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 13141 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 206/206 [15:42<00:00,  4.57s/it]\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "ValueError: Batch size of 13141 is greater than max batch size of 5461",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m         batch_embs = embedder.encode(batch, show_progress_bar=\u001b[38;5;28;01mFalse\u001b[39;00m).tolist()\n\u001b[32m     17\u001b[39m         all_embs.extend(batch_embs)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_embs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIndexed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\balog\\Desktop\\rag-scientific-qa\\.venv\\Lib\\site-packages\\chromadb\\api\\models\\Collection.py:115\u001b[39m, in \u001b[36mCollection.add\u001b[39m\u001b[34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m    ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \n\u001b[32m    104\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    106\u001b[39m add_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_add_request(\n\u001b[32m    107\u001b[39m     ids=ids,\n\u001b[32m    108\u001b[39m     embeddings=embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m     uris=uris,\n\u001b[32m    113\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_add\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadatas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muris\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\balog\\Desktop\\rag-scientific-qa\\.venv\\Lib\\site-packages\\chromadb\\api\\rust.py:452\u001b[39m, in \u001b[36mRustBindingsAPI._add\u001b[39m\u001b[34m(self, ids, collection_id, embeddings, metadatas, documents, uris, tenant, database)\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add\u001b[39m(\n\u001b[32m    432\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m    441\u001b[39m ) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    442\u001b[39m     \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    443\u001b[39m         CollectionAddEvent(\n\u001b[32m    444\u001b[39m             collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    449\u001b[39m         )\n\u001b[32m    450\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInternalError\u001b[39m: ValueError: Batch size of 13141 is greater than max batch size of 5461"
     ]
    }
   ],
   "source": [
    "# --- 4. Embed and index chunks ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Using smaller, faster SBERT model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "if collection.count() == 0:\n",
    "    print(f\"Embedding and indexing {len(chunk_texts)} chunks...\")\n",
    "    \n",
    "    # Embed and add in batches (ChromaDB has max batch size ~5000)\n",
    "    batch_size = 500\n",
    "    for i in tqdm(range(0, len(chunk_texts), batch_size), desc=\"Indexing\"):\n",
    "        batch_texts = chunk_texts[i:i + batch_size]\n",
    "        batch_metas = metadatas[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        \n",
    "        batch_embs = embedder.encode(batch_texts, show_progress_bar=False).tolist()\n",
    "        \n",
    "        collection.add(\n",
    "            documents=batch_texts,\n",
    "            embeddings=batch_embs,\n",
    "            metadatas=batch_metas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "    \n",
    "    print(f\"Indexed {collection.count()} chunks\")\n",
    "else:\n",
    "    print(f\"Collection already has {collection.count()} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bf673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Retrieval function ---\n",
    "def retrieve(query, k=3):\n",
    "    q_emb = embedder.encode([query]).tolist()[0]\n",
    "    results = collection.query(query_embeddings=[q_emb], n_results=k)\n",
    "    return results[\"documents\"][0], results[\"metadatas\"][0]\n",
    "\n",
    "# Test retrieval\n",
    "docs, metas = retrieve(\"random walk on networks\")\n",
    "for i, (doc, meta) in enumerate(zip(docs, metas)):\n",
    "    print(f\"[{i+1}] {meta['article_id']} (chunk {meta['chunk_idx']})\")\n",
    "    print(f\"    {doc[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9570c32",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- 6. Load LLM ---\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available, otherwise CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "print(f\"Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e40c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. RAG answer generation ---\n",
    "def rag_answer(query):\n",
    "    docs, metas = retrieve(query)\n",
    "    \n",
    "    # Build context with source references\n",
    "    context_parts = []\n",
    "    for i, (doc, meta) in enumerate(zip(docs, metas), 1):\n",
    "        context_parts.append(f\"[{i}] Source: {meta['article_id']}\\n{doc}\")\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    prompt = f\"\"\"Use the following context to answer the question. Cite sources using [1], [2], etc.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**tokens, max_new_tokens=200, do_sample=False)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the answer part\n",
    "    if \"ANSWER:\" in answer:\n",
    "        answer = answer.split(\"ANSWER:\")[-1].strip()\n",
    "    \n",
    "    # Return answer and sources\n",
    "    sources = [meta['article_id'] for meta in metas]\n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a65dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Test queries ---\n",
    "test_queries = [\n",
    "    \"What is a random walk on a network?\",\n",
    "    \"How do biased random walks work?\",\n",
    "    \"What is entropy rate?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"QUESTION: {q}\")\n",
    "    answer, sources = rag_answer(q)\n",
    "    print(f\"ANSWER: {answer.split('ANSWER:')[-1].strip()}\")\n",
    "    print(f\"SOURCES: {', '.join(sources)}\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-scientific-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
