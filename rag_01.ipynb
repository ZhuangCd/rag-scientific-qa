{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41125219",
   "metadata": {},
   "source": [
    "# Simple RAG for Scientific Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ced6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.5 environment at: C:\\Users\\balog\\AppData\\Local\\Programs\\Python\\Python312\u001b[0m\n",
      "  \u001b[31m×\u001b[0m Failed to read `websocket-client==1.8.0`\n",
      "\u001b[31m  ├─▶ \u001b[0mFailed to read metadata from installed package `websocket-client==1.8.0`\n",
      "\u001b[31m  ╰─▶ \u001b[0mfailed to open file\n",
      "\u001b[31m      \u001b[0m`C:\\Users\\balog\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\websocket_client-1.8.0.dist-info\\METADATA`:\n",
      "\u001b[31m      \u001b[0mThe system cannot find the file specified. (os error 2)\n",
      "\u001b[36m  help: \u001b[0m`\u001b[36mwebsocket-client\u001b[39m` (\u001b[36mv1.8.0\u001b[39m) was included because `\u001b[36mchromadb\u001b[39m`\n",
      "        (\u001b[36mv1.4.1\u001b[39m) depends on `\u001b[36mkubernetes\u001b[39m\u001b[36m>=28.1.0\u001b[39m` (\u001b[36mv35.0.0\u001b[39m) which depends\n",
      "        on `\u001b[36mwebsocket-client\u001b[39m\u001b[36m>=0.32.0, <0.40.0 | >0.40.0, <0.41.dev0 |\n",
      "        >=0.43.dev0\u001b[39m`\n"
     ]
    }
   ],
   "source": [
    "#uv pip install chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f14054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 papers\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load scientific papers from JSON ---\n",
    "import json\n",
    "import os\n",
    "\n",
    "papers_dir = \"../papers_json_3/papers_json_3\"\n",
    "\n",
    "corpus = []\n",
    "files = sorted([f for f in os.listdir(papers_dir) if f.endswith('.json')])[:200]\n",
    "\n",
    "for filename in files:\n",
    "    with open(os.path.join(papers_dir, filename), 'r', encoding='utf-8') as f:\n",
    "        paper = json.load(f)\n",
    "    corpus.append({\n",
    "        \"article_id\": paper.get(\"article_id\", filename.replace(\".json\", \"\")),\n",
    "        \"text\": paper.get(\"abstract\", \"\") + \"\\n\\n\" + paper.get(\"article\", \"\")\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(corpus)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 13141\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Overlapping chunking (window 150, overlap 50, only if > 100 tokens) ---\n",
    "\n",
    "def chunk_text(text, chunk_size=150, overlap=50):\n",
    "    words = text.split()\n",
    "    if len(words) <= 100:\n",
    "        return [text] if words else []\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Build chunks with metadata\n",
    "chunk_texts, metadatas, ids = [], [], []\n",
    "\n",
    "for paper in corpus:\n",
    "    chunks = chunk_text(paper[\"text\"])\n",
    "    for idx, ch in enumerate(chunks):\n",
    "        chunk_texts.append(ch)\n",
    "        metadatas.append({\"article_id\": paper[\"article_id\"], \"chunk_idx\": idx})\n",
    "        ids.append(f'{paper[\"article_id\"]}_chunk_{idx}')\n",
    "\n",
    "print(f\"Total chunks: {len(chunk_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c9fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Initialize ChromaDB ---\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"scientific_rag_db\")\n",
    "collection = client.get_or_create_collection(\"scientific_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2be9fccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding and indexing 13141 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing: 100%|██████████| 27/27 [16:32<00:00, 36.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 13141 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Embed and index chunks ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Using smaller, faster SBERT model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "if collection.count() == 0:\n",
    "    print(f\"Embedding and indexing {len(chunk_texts)} chunks...\")\n",
    "    \n",
    "    # Embed and add in batches (ChromaDB has max batch size ~5000)\n",
    "    batch_size = 500\n",
    "    for i in tqdm(range(0, len(chunk_texts), batch_size), desc=\"Indexing\"):\n",
    "        batch_texts = chunk_texts[i:i + batch_size]\n",
    "        batch_metas = metadatas[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        \n",
    "        batch_embs = embedder.encode(batch_texts, show_progress_bar=False).tolist()\n",
    "        \n",
    "        collection.add(\n",
    "            documents=batch_texts,\n",
    "            embeddings=batch_embs,\n",
    "            metadatas=batch_metas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "    \n",
    "    print(f\"Indexed {collection.count()} chunks\")\n",
    "else:\n",
    "    print(f\"Collection already has {collection.count()} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc680787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RELOAD: Use this cell to reload everything without re-indexing if default saving is not working---\n",
    "# Run this cell alone next time instead of cells 3-6\n",
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Reload ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"scientific_rag_db\")\n",
    "collection = client.get_collection(\"scientific_papers\")\n",
    "print(f\"Loaded {collection.count()} chunks from disk\")\n",
    "\n",
    "# Reload embedder\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedder loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2bf673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] article_1 (chunk 6)\n",
      "    occupation probability , and coverage , have tight relationships with the structure of the graph upon which the walk takes place @xcite . for this rea...\n",
      "\n",
      "[2] article_1 (chunk 9)\n",
      "    walkers on the topological properties of the nodes at each layer in order to perform an efficient exploration of such systems . we notice that random ...\n",
      "\n",
      "[3] article_1 (chunk 8)\n",
      "    communities @xcite , and provide optimal exploration of a network using only local information @xcite . it has also been found that the dynamics of de...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Retrieval function ---\n",
    "def retrieve(query, k=3):\n",
    "    q_emb = embedder.encode([query]).tolist()[0]\n",
    "    results = collection.query(query_embeddings=[q_emb], n_results=k)\n",
    "    return results[\"documents\"][0], results[\"metadatas\"][0]\n",
    "\n",
    "# Test retrieval\n",
    "docs, metas = retrieve(\"random walk on networks\")\n",
    "for i, (doc, meta) in enumerate(zip(docs, metas)):\n",
    "    print(f\"[{i+1}] {meta['article_id']} (chunk {meta['chunk_idx']})\")\n",
    "    print(f\"    {doc[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9570c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\balog\\Desktop\\rag-scientific-qa\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\balog\\Desktop\\rag-scientific-qa\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\balog\\Desktop\\rag-scientific-qa\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4806\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4804\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDeepSpeed Zero-3 is not compatible with passing a `device_map`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4805\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m4806\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4807\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUsing a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4808\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequires `accelerate`. You can install it with `pip install accelerate`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4809\u001b[39m         )\n\u001b[32m   4811\u001b[39m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[32m   4812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[31mValueError\u001b[39m: Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "# --- 6. Load LLM ---\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available, otherwise CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "else:\n",
    "    # CPU: load without device_map\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e40c44",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- 7. RAG answer generation ---\n",
    "def rag_answer(query):\n",
    "    docs, metas = retrieve(query)\n",
    "    \n",
    "    # Build context with source references\n",
    "    context_parts = []\n",
    "    for i, (doc, meta) in enumerate(zip(docs, metas), 1):\n",
    "        context_parts.append(f\"[{i}] Source: {meta['article_id']}\\n{doc}\")\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    prompt = f\"\"\"Use the following context to answer the question. Cite sources using [1], [2], etc.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**tokens, max_new_tokens=200, do_sample=False)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the answer part\n",
    "    if \"ANSWER:\" in answer:\n",
    "        answer = answer.split(\"ANSWER:\")[-1].strip()\n",
    "    \n",
    "    # Return answer and sources\n",
    "    sources = [meta['article_id'] for meta in metas]\n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a65dcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- 8. Test queries ---\n",
    "test_queries = [\n",
    "    \"What is a random walk on a network?\",\n",
    "    \"How do biased random walks work?\",\n",
    "    \"What is entropy rate?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"QUESTION: {q}\")\n",
    "    answer, sources = rag_answer(q)\n",
    "    print(f\"ANSWER: {answer.split('ANSWER:')[-1].strip()}\")\n",
    "    print(f\"SOURCES: {', '.join(sources)}\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-scientific-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
