{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41125219",
   "metadata": {},
   "source": [
    "# Simple RAG for Scientific Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ced6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.5 environment at: C:\\Users\\balog\\AppData\\Local\\Programs\\Python\\Python312\u001b[0m\n",
      "  \u001b[31m×\u001b[0m Failed to read `websocket-client==1.8.0`\n",
      "\u001b[31m  ├─▶ \u001b[0mFailed to read metadata from installed package `websocket-client==1.8.0`\n",
      "\u001b[31m  ╰─▶ \u001b[0mfailed to open file\n",
      "\u001b[31m      \u001b[0m`C:\\Users\\balog\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\websocket_client-1.8.0.dist-info\\METADATA`:\n",
      "\u001b[31m      \u001b[0mThe system cannot find the file specified. (os error 2)\n",
      "\u001b[36m  help: \u001b[0m`\u001b[36mwebsocket-client\u001b[39m` (\u001b[36mv1.8.0\u001b[39m) was included because `\u001b[36mchromadb\u001b[39m`\n",
      "        (\u001b[36mv1.4.1\u001b[39m) depends on `\u001b[36mkubernetes\u001b[39m\u001b[36m>=28.1.0\u001b[39m` (\u001b[36mv35.0.0\u001b[39m) which depends\n",
      "        on `\u001b[36mwebsocket-client\u001b[39m\u001b[36m>=0.32.0, <0.40.0 | >0.40.0, <0.41.dev0 |\n",
      "        >=0.43.dev0\u001b[39m`\n"
     ]
    }
   ],
   "source": [
    "#uv pip install chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f14054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 papers\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load scientific papers from JSON ---\n",
    "import json\n",
    "import os\n",
    "\n",
    "papers_dir = \"../papers_json_3/papers_json_3\"\n",
    "\n",
    "corpus = []\n",
    "files = sorted([f for f in os.listdir(papers_dir) if f.endswith('.json')])[:200]\n",
    "\n",
    "for filename in files:\n",
    "    with open(os.path.join(papers_dir, filename), 'r', encoding='utf-8') as f:\n",
    "        paper = json.load(f)\n",
    "    corpus.append({\n",
    "        \"article_id\": paper.get(\"article_id\", filename.replace(\".json\", \"\")),\n",
    "        \"text\": paper.get(\"abstract\", \"\") + \"\\n\\n\" + paper.get(\"article\", \"\")\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(corpus)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 13141\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Overlapping chunking (window 150, overlap 50, only if > 100 tokens) ---\n",
    "\n",
    "def chunk_text(text, chunk_size=150, overlap=50):\n",
    "    words = text.split()\n",
    "    if len(words) <= 100:\n",
    "        return [text] if words else []\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Build chunks with metadata\n",
    "chunk_texts, metadatas, ids = [], [], []\n",
    "\n",
    "for paper in corpus:\n",
    "    chunks = chunk_text(paper[\"text\"])\n",
    "    for idx, ch in enumerate(chunks):\n",
    "        chunk_texts.append(ch)\n",
    "        metadatas.append({\"article_id\": paper[\"article_id\"], \"chunk_idx\": idx})\n",
    "        ids.append(f'{paper[\"article_id\"]}_chunk_{idx}')\n",
    "\n",
    "print(f\"Total chunks: {len(chunk_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7c9fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Initialize ChromaDB ---\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"scientific_rag_db\")\n",
    "collection = client.get_or_create_collection(\"scientific_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be9fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Embed and index chunks ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Using SBERT (Sentence-BERT) model\n",
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "if collection.count() == 0:\n",
    "    embs = embedder.encode(chunk_texts, batch_size=32).tolist()\n",
    "    collection.add(\n",
    "        documents=chunk_texts,\n",
    "        embeddings=embs,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids\n",
    "    )\n",
    "    print(f\"Indexed {collection.count()} chunks\")\n",
    "else:\n",
    "    print(f\"Collection already has {collection.count()} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2bf673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Retrieval function ---\n",
    "def retrieve(query, k=3):\n",
    "    q_emb = embedder.encode([query]).tolist()[0]\n",
    "    results = collection.query(query_embeddings=[q_emb], n_results=k)\n",
    "    return results[\"documents\"][0], results[\"metadatas\"][0]\n",
    "\n",
    "# Test retrieval\n",
    "docs, metas = retrieve(\"random walk on networks\")\n",
    "for i, (doc, meta) in enumerate(zip(docs, metas)):\n",
    "    print(f\"[{i+1}] {meta['article_id']} (chunk {meta['chunk_idx']})\")\n",
    "    print(f\"    {doc[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9570c32",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# --- 6. Load LLM ---\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available, otherwise CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "print(f\"Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e40c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. RAG answer generation ---\n",
    "def rag_answer(query):\n",
    "    docs, metas = retrieve(query)\n",
    "    \n",
    "    # Build context with source references\n",
    "    context_parts = []\n",
    "    for i, (doc, meta) in enumerate(zip(docs, metas), 1):\n",
    "        context_parts.append(f\"[{i}] Source: {meta['article_id']}\\n{doc}\")\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    prompt = f\"\"\"Use the following context to answer the question. Cite sources using [1], [2], etc.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**tokens, max_new_tokens=200, do_sample=False)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the answer part\n",
    "    if \"ANSWER:\" in answer:\n",
    "        answer = answer.split(\"ANSWER:\")[-1].strip()\n",
    "    \n",
    "    # Return answer and sources\n",
    "    sources = [meta['article_id'] for meta in metas]\n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a65dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Test queries ---\n",
    "test_queries = [\n",
    "    \"What is a random walk on a network?\",\n",
    "    \"How do biased random walks work?\",\n",
    "    \"What is entropy rate?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"QUESTION: {q}\")\n",
    "    answer, sources = rag_answer(q)\n",
    "    print(f\"ANSWER: {answer.split('ANSWER:')[-1].strip()}\")\n",
    "    print(f\"SOURCES: {', '.join(sources)}\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-scientific-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
