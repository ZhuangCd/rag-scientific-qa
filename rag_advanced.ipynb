{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41125219",
   "metadata": {},
   "source": [
    "# Advanced RAG for Scientific Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "89f5eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# print(\"TF:\", tf.__version__)\n",
    "# print(\"Devices:\", tf.config.list_physical_devices())\n",
    "# print(\"GPUs:\", tf.config.list_physical_devices(\"GPU\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d2ced6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uv pip install chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6f14054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 papers\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load scientific papers from JSON ---\n",
    "import json\n",
    "import os\n",
    "\n",
    "papers_dir = \"../papers_json_3\"\n",
    "\n",
    "corpus = []\n",
    "files = sorted([f for f in os.listdir(papers_dir) if f.endswith('.json')])[:200]\n",
    "\n",
    "for filename in files:\n",
    "    with open(os.path.join(papers_dir, filename), 'r', encoding='utf-8') as f:\n",
    "        paper = json.load(f)\n",
    "    corpus.append({\n",
    "        \"article_id\": paper.get(\"article_id\", filename.replace(\".json\", \"\")),\n",
    "        \"text\": paper.get(\"abstract\", \"\") + \"\\n\\n\" + paper.get(\"article\", \"\")\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(corpus)} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d837275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 3830\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Improved Chunking Strategy (larger chunks, more overlap) ---\n",
    "'''\n",
    "Advanced chunking: chunk_size=450 words, overlap=100 words\n",
    "Larger chunks preserve more context for scientific papers\n",
    "'''\n",
    "def chunk_text(text, chunk_size=450, overlap=100):\n",
    "    words = text.split()\n",
    "    if len(words) <= 100:\n",
    "        return [text] if words else []\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Build chunks with metadata\n",
    "chunk_texts, metadatas, ids = [], [], []\n",
    "\n",
    "for paper in corpus:\n",
    "    chunks = chunk_text(paper[\"text\"])\n",
    "    for idx, ch in enumerate(chunks):\n",
    "        chunk_texts.append(ch)\n",
    "        metadatas.append({\"article_id\": paper[\"article_id\"], \"chunk_idx\": idx})\n",
    "        ids.append(f'{paper[\"article_id\"]}_chunk_{idx}')\n",
    "\n",
    "print(f\"Total chunks: {len(chunk_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c7c9fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Initialize ChromaDB ---\n",
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"scientific_rag_db\")\n",
    "collection = client.get_or_create_collection(\"scientific_papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2be9fccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection already has 3830 chunks\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Embed and index chunks with BAAI model ---\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Using BAAI/bge-base-en-v1.5 - state-of-the-art for retrieval\n",
    "embedder = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "if collection.count() == 0:\n",
    "    print(f\"Embedding and indexing {len(chunk_texts)} chunks...\")\n",
    "    \n",
    "    # Embed and add in batches (ChromaDB has max batch size ~5000)\n",
    "    batch_size = 500\n",
    "    for i in tqdm(range(0, len(chunk_texts), batch_size), desc=\"Indexing\"):\n",
    "        batch_texts = chunk_texts[i:i + batch_size]\n",
    "        batch_metas = metadatas[i:i + batch_size]\n",
    "        batch_ids = ids[i:i + batch_size]\n",
    "        \n",
    "        batch_embs = embedder.encode(batch_texts, show_progress_bar=False).tolist()\n",
    "        \n",
    "        collection.add(\n",
    "            documents=batch_texts,\n",
    "            embeddings=batch_embs,\n",
    "            metadatas=batch_metas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "    \n",
    "    print(f\"Indexed {collection.count()} chunks\")\n",
    "else:\n",
    "    print(f\"Collection already has {collection.count()} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8b2bf673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] article_1 (chunk 5)\n",
      "    not be directly mapped on an equivalent walk on the aggregated graph . _ stationary probability distribution . _ starting from the one - step transiti...\n",
      "\n",
      "[2] article_1 (chunk 0)\n",
      "    efficient techniques to navigate networks with local information are fundamental to sample large - scale online social systems and to retrieve resourc...\n",
      "\n",
      "[3] article_102 (chunk 42)\n",
      "    and sons , 2008 , xv+352 pages . s. janson , t. uczak , t. turova , and t. vallier . bootstrap percolation on the random graph @xmath858 . , 22(5):198...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Basic Retrieval function ---\n",
    "def retrieve(query, k=3):\n",
    "    q_emb = embedder.encode([query]).tolist()[0]\n",
    "    results = collection.query(query_embeddings=[q_emb], n_results=k)\n",
    "    return results[\"documents\"][0], results[\"metadatas\"][0], results[\"ids\"][0]\n",
    "\n",
    "# Test retrieval\n",
    "docs, metas, doc_ids = retrieve(\"random walk on networks\")\n",
    "for i, (doc, meta) in enumerate(zip(docs, metas)):\n",
    "    print(f\"[{i+1}] {meta['article_id']} (chunk {meta['chunk_idx']})\")\n",
    "    print(f\"    {doc[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d9570c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model loaded: Qwen/Qwen2.5-1.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Load LLM ---\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Use GPU if available, otherwise CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "print(f\"Model loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "372509d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Multi-Query Rewriting ---\n",
    "def generate_alternative_queries(question, n_queries=3):\n",
    "    prompt = (\n",
    "        f\"Rewrite the following question into {n_queries} alternative search queries, \"\n",
    "        f\"each on a new line:\\n\\nQuestion: {question}\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inputs, max_new_tokens=150)\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    lines = [l.strip(\"-â€¢ \").strip() for l in text.split(\"\\n\") if l.strip()]\n",
    "    lines = [l for l in lines if question.lower() not in l.lower()]\n",
    "\n",
    "    return lines[:n_queries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "05755576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Multi-Query Retrieval with Deduplication ---\n",
    "def retrieve_multi_query(question, k_per_query=3, n_queries=3):\n",
    "    mq = generate_alternative_queries(question, n_queries=n_queries)\n",
    "    all_qs = [question] + mq\n",
    "\n",
    "    seen = set()\n",
    "    results_list = []\n",
    "\n",
    "    for q in all_qs:\n",
    "        q_emb = embedder.encode([q]).tolist()[0]\n",
    "        result = collection.query(\n",
    "            query_embeddings=[q_emb],\n",
    "            n_results=k_per_query\n",
    "        )\n",
    "\n",
    "        for doc_id, doc, meta in zip(result[\"ids\"][0], result[\"documents\"][0], result[\"metadatas\"][0]):\n",
    "            if doc_id not in seen:\n",
    "                seen.add(doc_id)\n",
    "                results_list.append({\"id\": doc_id, \"text\": doc, \"metadata\": meta})\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4ab443fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Context Building with Character Limits ---\n",
    "def build_context_with_limits(docs, limit_chars=3500):\n",
    "    context = []\n",
    "    total = 0\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        header = f\"[{i}] Source: {d['metadata']['article_id']} (chunk {d['metadata']['chunk_idx']})\"\n",
    "        block = f\"{header}\\n{d['text'][:900]}\\n\"\n",
    "        if total + len(block) > limit_chars:\n",
    "            break\n",
    "        context.append(block)\n",
    "        total += len(block)\n",
    "    return \"\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f8e40c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 9. Advanced RAG Answer Generation ---\n",
    "def rag_answer(query, use_multi_query=True):\n",
    "    if use_multi_query:\n",
    "        retrieved = retrieve_multi_query(query)\n",
    "        context = build_context_with_limits(retrieved)\n",
    "        sources = [d['metadata']['article_id'] for d in retrieved]\n",
    "    else:\n",
    "        docs, metas, _ = retrieve(query)\n",
    "        context_parts = []\n",
    "        for i, (doc, meta) in enumerate(zip(docs, metas), 1):\n",
    "            context_parts.append(f\"[{i}] Source: {meta['article_id']} (chunk {meta['chunk_idx']})\\n{doc[:900]}\")\n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        sources = [meta['article_id'] for meta in metas]\n",
    "\n",
    "    prompt = f\"\"\"You are an expert scientific research assistant specializing in analyzing academic papers.\n",
    "\n",
    "TASK: Answer the user's question using ONLY the information from the provided sources below. \n",
    "\n",
    "STRICT RULES:\n",
    "1. Base your answer EXCLUSIVELY on the provided sources - do not use external knowledge.\n",
    "2. Cite sources using [\"article_id\"]  corresponding to the source numbers.\n",
    "3. If the sources do not contain enough information to answer the question, explicitly state: \"The provided sources do not contain sufficient information to answer this question.\"\n",
    "4. Be precise, accurate, and use scientific terminology appropriately.\n",
    "5. When sources contain conflicting information, acknowledge the discrepancy and cite both.\n",
    "\n",
    "==================== SOURCES ====================\n",
    "{context}\n",
    "=================================================\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER (with citations):\"\"\"\n",
    "\n",
    "    tokens = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**tokens, max_new_tokens=250, do_sample=False)\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the answer part\n",
    "    if \"ANSWER (with citations):\" in answer:\n",
    "        answer = answer.split(\"ANSWER (with citations):\")[-1].strip()\n",
    "    \n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "79a65dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Where is the jet energy dissipated relative to the cool core in the system discussed?\n",
      "\n",
      "ANSWER: The jet energy is primarily dissipated outside the cool core region, with approximately 35% being lost through shock waves and turbulence, and the remaining 65% being transported to regions beyond 33 kpc. Notably, some of the jet energy is also dissipated outside the cool core, indicating a significant redistribution of energy across the cluster. This suggests that the jet's impact extends well beyond the immediate vicinity of the cool core, highlighting the complex dynamics involved in AGN feedback mechanisms. [\"source_4\", \"source_5\"]\n",
      "\n",
      "SOURCES: article_63, article_141, article_4, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: What determines the actual sensitivity of the observational setup mentioned?\n",
      "\n",
      "ANSWER: The actual sensitivity of the observational setup mentioned depends on multiple factors including the quality of the instruments used, the accuracy of the calibration processes, and the specific conditions under which observations are conducted. Sources [1], [2], and [3] discuss how the performance of instruments like WFPC2 and ACS (Advanced Camera for Surveys) is characterized and calibrated, highlighting the importance of detailed comparisons across different observation scenarios. These studies emphasize the need for consistent methods and thorough testing to ensure reliable measurement outcomes. Therefore, while the exact parameters determining sensitivity may vary depending on the specific application or environment, the general principles outlined in these sources underscore the critical role of precision instrumentation and rigorous calibration protocols in achieving high-quality astronomical observations. [\"article_1\"], [\"article_2\"], [\"article_3\"]\n",
      "\n",
      "SOURCES: article_63, article_46, article_141, article_73, article_93, article_161, article_36\n",
      "================================================================================\n",
      "QUESTION: What are the two possible mechanisms by which photons may be produced in the described process?\n",
      "\n",
      "ANSWER: The two possible mechanisms by which photons may be produced in the described process are:\n",
      "\n",
      "1. Bremsstrahlung emission of a photon, which occurs through the interaction of a photon with a hard quark in a scattering process where the quark is backward scattered in the plasma via soft quark exchange. This mechanism appears at the two-loop level in the effective theory and contributes at the same order as the processes in the one-loop effective theory.\n",
      "\n",
      "2. Processes where the photon is radiated off fast quarks in scattering processes mediated by a gluon exchange, which is referred to as bremsstrahlung emission of a photon in the context of the semi-classical approximation. These processes favor the semi-classical approximation and contribute at the same order in the effective theory. [Source: article_118 (chunk 3)]\n",
      "\n",
      "SOURCES: article_118, article_63, article_141, article_196, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: How are sources and destinations paired in the network model?\n",
      "\n",
      "ANSWER: The sources and destinations in the network model are paired based on the connectivity of the edges within the graph. Specifically, each node represents a destination or source location, and the edges represent the pathways connecting these locations. In the context of transportation networks, such as airlines, the goal is to ensure that there are multiple routes available between any pair of distant locations, which enhances accessibility and reduces the need for long-distance flights. This approach aims to balance the need for efficient travel with the requirement for robustness against potential disruptions like single-point failures. The sources and destinations are thus interconnected through various edges, forming a complex network structure that reflects the practical challenges and benefits of designing efficient yet resilient transportation systems. [\"article_1\"]\n",
      "\n",
      "Sources and destinations are also paired in the context of edge overlap analysis. High edge overlap in a network indicates that many pairs of nodes are directly connected by several edges, leading to reduced dispersiveness of the walk and increased heterogeneity in the equilibrium occupation probability distribution. This configuration suggests that the network is designed to maximize robustness while maintaining a certain level of diffusivity. Edge overlap is crucial in understanding how different layers of a transportation system interact and contribute to its overall performance. [\"article_1\"]\n",
      "\n",
      "SOURCES: article_47, article_103, article_63, article_20, article_1, article_171, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: What conclusion is reached about the consistency of the constraint algebra for quantum Bohmian trajectories?\n",
      "\n",
      "ANSWER: The provided sources do not contain sufficient information to answer this question. [source: article_198 (chunk 14)]\n",
      "\n",
      "SOURCES: article_47, article_198, article_63, article_20, article_171, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: On what observational sample size and survey are the presented galaxy results based?\n",
      "\n",
      "ANSWER: The presented galaxy results are based on observations from the Sloan Digital Sky Survey (SDSS) survey. Specifically, the authors compare their sample to the SDSS sample, using a visual (or in case of the SDSS, trained on visual) morphological classification. This comparison allows them to analyze the properties of galaxies across various scales and environments, including those in clusters and groups. Additionally, the study involves fitting data to theoretical models and performing statistical tests to validate the findings against existing literature such as Valentinuzzi et al.'s work on the overall mass-size relation. These methods collectively provide a comprehensive understanding of galaxy morphology and evolution within different cosmic structures. [\"source_1\"] [\"source_2\"] [\"source_3\"]\n",
      "\n",
      "SOURCES: article_63, article_141, article_46, article_173, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: Which theoretical framework motivates the idea that the universe contains many light scalar fields?\n",
      "\n",
      "ANSWER: The idea that the universe contains many light scalar fields is motivated by landscape models, as discussed in reference [1]. Landscape models propose that there exist multiple vacua or stable configurations of the universe, each with different values of certain parameters. These diverse vacua suggest that the universe may have evolved through a series of transitions between them, driven by the interactions among the scalar fields. This concept provides a natural explanation for the observed fine-tuning of fundamental constants and the existence of multiple forms of matter and energy within the universe. Additionally, the work in references [2] and [3] further supports this idea by exploring scenarios involving multiple scalar fields interacting in complex ways, such as in assisted inflation and multi-field dynamics, which collectively contribute to the rich structure of the universe. Therefore, the motivation for considering many light scalar fields comes from the broader landscape paradigm in theoretical physics. [\"article_2\"], [\"article_3\"]\n",
      "\n",
      "SOURCES: article_47, article_63, article_20, article_171, article_23, article_2, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: Why is the mentioned gamma-ray binary system considered particularly interesting?\n",
      "\n",
      "ANSWER: The gamma-ray binary system PSR B1259-63/LS 2883 is considered particularly interesting due to its unique feature of having the nature of the compact object known. This makes it one of the few systems where the identity of the neutron star or black hole can be determined through observational means. Additionally, the presence of both pulsed and non-pulsed emissions, as well as the detection of infrared excesses, adds complexity to the system's behavior, making it a subject of ongoing investigation into the physics of such binaries. Furthermore, the variability of the emitted radiation over different wavelengths, including radio, X-rays, and TeV gamma rays, suggests dynamic processes occurring within the system, potentially involving interactions between the pulsar, stellar winds, and surrounding matter. These factors collectively make the system a valuable target for studying fundamental astrophysical phenomena. [\"source_1\", \"source_2\"]\n",
      "\n",
      "SOURCES: article_63, article_141, article_33, article_99, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: What key assumption about turbulence underlies the basis of the present work on local equilibrium patches?\n",
      "\n",
      "ANSWER: The key assumption underlying the basis of the present work on local equilibrium patches is that local equilibrium patches are embedded in turbulence. This concept is rooted in the understanding that turbulence can lead to the formation of stable, localized structures or patches within the broader context of chaotic dynamics. These equilibrium patches are thought to arise through the process of relaxation, where the system tends towards a state of balance despite ongoing random fluctuations. This approach contrasts with traditional models of turbulence, which often focus on the large-scale behavior of fluids without considering the role of smaller-scale structures. By recognizing the importance of these local equilibrium patches, researchers aim to better understand how turbulence affects the overall structure and dynamics of fluid flows. [\"source: article_154(chunk 1)\"]\n",
      "\n",
      "This perspective allows for a more comprehensive view of turbulence, acknowledging its multifaceted nature and the complex interplay between larger-scale motions and smaller-scale structures. It also opens up new avenues for studying phenomena like alignment effects, which are crucial for understanding various physical processes across different scales. [\"source: article_154(chunk 8)\"]\n",
      "\n",
      "SOURCES: article_91, article_63, article_154, article_141, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: How does the mass-ratio distribution of stellar companions to hot Jupiter systems compare to that of field star binaries?\n",
      "\n",
      "ANSWER: To compare the mass-ratio distribution of stellar companions to hot Jupiter systems with that of field star binaries, we examine Figure [fig: eta_mratio], which plots the survey's observed companion fraction, the survey's completeness-corrected companion fraction, and the completeness-corrected field star companion fraction as a function of companion star mass ratio. The data points show that the distribution of mass ratios for the stellar companions detected in our survey is concentrated towards smaller values, indicating a lower prevalence of high-mass companions relative to field stars. This suggests that the mass-ratio distribution of stellar companions to hot Jupiter systems may be more consistent with the expected behavior of field star binaries rather than being significantly different. However, further analysis would be needed to draw definitive conclusions about the specific differences or similarities between the two populations. [\"article_55\", chunk 19]\n",
      "\n",
      "This comparison indicates that while there might be some overlap in terms of overall trends, the specific characteristics of the mass-ratio distributions suggest that the stellar companions to hot Jupiters could potentially follow a pattern more aligned with field star binaries. Further detailed examination of the data and additional observations would be necessary to make a more comprehensive assessment. [\"article_55\", chunk 19\n",
      "\n",
      "SOURCES: article_47, article_63, article_20, article_55, article_171, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: Which numerical model is employed for microswimmers, and how are the swimmers represented in this model?\n",
      "\n",
      "ANSWER: The numerical model employed for microswimmers is based on the Smoothed Particle Hydrodynamics (SPH) scheme. In this model, the walls of the confined space are represented by assemblies of particles that are pinned and cannot move or rotate. These particles are placed uniformly throughout the computational domain and have the same diameter as the swimmer particles. This approach ensures the creation of a stick boundary at the wall surfaces, thereby guaranteeing the stability of the simulation. By considering various shapes and sizes of pipes, the number of wall particles needed increases accordingly. The dynamics of these microswimmers under confinement are studied through three-dimensional direct numerical simulations with fully resolved hydrodynamics. The results indicate that while the volume fraction dependency may differ qualitatively, the overall behavior does not significantly change depending on the geometry of the confinement. An order-disorder phase transition occurs for certain types of swimmers when their size approaches the scale of the swimmers themselves. This phenomenon is independent of the specific geometry of the confinement environment. Self-propelled particles (SPPs), which include microswimmers, are increasingly being recognized as representatives of out-of-equilibrium systems due to their unique properties and behaviors. For further reading, refer to articles [4] and [5\n",
      "\n",
      "SOURCES: article_63, article_9, article_141, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: Why is the magnetic moment considered a fundamental property of the nucleon?\n",
      "\n",
      "ANSWER: The magnetic moment is considered a fundamental property of the nucleon because it plays a crucial role in understanding its behavior within the framework of quantum chromodynamics (QCD). As stated in the given sources, the magnetic moment is one of the fundamental properties of the nucleon, and experimental measurements have been performed to a very high precision. This measurement provides valuable insights into the internal structure of the nucleon and helps validate theoretical models that aim to explain its properties. Additionally, the magnetic moment serves as a benchmark for testing various theoretical approaches and computational methods used in QCD studies. Therefore, its determination is essential for advancing our understanding of hadronic physics and the underlying principles governing the strong interaction between quarks. [\"source_1\", \"source_2\"]\n",
      "\n",
      "SOURCES: article_63, article_141, article_98, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: Under what restriction does the algebra of constraints remain closed, avoiding inconsistency in quantum geometrodynamics and quantum field theory?\n",
      "\n",
      "ANSWER: The algebra of constraints remains closed under the condition that the quantum potential does not break Dirac-Titelboim's algebra. This implies that the constraints are conserved in time, preventing inconsistencies in Hamiltonian evolution of spacelike geometries. In quantum field theory, this scenario can occur if the quantum potential modifies the background spacetime, leading to a breakdown of Lorentz invariance. Specifically, the ground state of the free scalar field introduces a quantum potential that violates Dirac-Titelboim's algebra, thus breaking Lorentz invariance. This phenomenon occurs within the context of the BDB (Bohm-Dewitt) interpretation of quantum cosmology, where the choice of quantum potential determines the quantum evolution of the universe. Therefore, the key restriction ensuring the closure of the algebra of constraints is the absence of a quantum potential that disrupts Dirac-Titelboim's algebraic structure. [\"article_198\", chunk 15]\n",
      "\n",
      "This restriction ensures that the mathematical framework of quantum geometrodynamics and quantum field theory remains consistent, maintaining the principles of Lorentz invariance and the conservation of physical laws across different scales and dimensions. [\"article_198\", chunk 15],\n",
      "\n",
      "SOURCES: article_198, article_63, article_141, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: What do clusters of galaxies trace in the context of large-scale cosmic structure?\n",
      "\n",
      "ANSWER: Clusters of galaxies trace the large-scale cosmic structure through their clustering properties. The study mentioned uses photometric redshifts and ACS GTO spectroscopic follow-up observations to select a cluster galaxy sample that remains unchanged compared to the group sample. This selection process allows for the analysis of the clustering behavior within the selected clusters and groups. The authors classify galaxies into three types: early-type (ETGs), late-type (SA, SB, SC), and irregular/undefined categories based on visual classifications derived from ACS @xmath38 images. These classifications help in understanding the morphological characteristics of galaxies within the selected samples. The work presented here involves measuring the spatial correlation lengths of galaxy clusters using data from the Las Campanas Distant Cluster Survey (LCDCS) and deriving them via the cosmological Limber equation. Despite the limitations due to statistical uncertainties, the measured correlation lengths provide insights into the large-scale distribution of matter in the universe. The findings suggest that clusters play a crucial role in tracing the large-scale cosmic structure, contributing significantly to our understanding of the distribution of dark matter and the evolution of the universe. [Sources: article_173, article_120]\n",
      "\n",
      "SOURCES: article_63, article_141, article_120, article_173, article_93, article_36\n",
      "================================================================================\n",
      "QUESTION: Why are radiative decays of heavy quarkonia useful for studying color-singlet two-gluon systems?\n",
      "\n",
      "ANSWER: Radiative decays of heavy quarkonia are useful for studying color-singlet two-gluon systems because they allow researchers to observe the interaction between a photon and two gluons within the same system. This interaction provides insights into the properties of these systems, particularly in terms of their structure and dynamics. The presence of a photon in the decay process changes the nature of the interaction compared to direct gluon decays, offering unique opportunities to probe the underlying physics of color-singlet configurations. Additionally, the suppression of rates due to the quark-photon coupling allows for more controlled studies of these processes, making them valuable tools in understanding the behavior of color-singlet two-gluon systems. [\"source_1\", \"source_2\"]\n",
      "\n",
      "SOURCES: article_63, article_87, article_141, article_93, article_31, article_36\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 10. Test Advanced RAG ---\n",
    "test_queries = [\n",
    "\"Where is the jet energy dissipated relative to the cool core in the system discussed?\",\n",
    "\"What determines the actual sensitivity of the observational setup mentioned?\",\n",
    "\"What are the two possible mechanisms by which photons may be produced in the described process?\",\n",
    "\"How are sources and destinations paired in the network model?\",\n",
    "\"What conclusion is reached about the consistency of the constraint algebra for quantum Bohmian trajectories?\",\n",
    "\"On what observational sample size and survey are the presented galaxy results based?\",\n",
    "\"Which theoretical framework motivates the idea that the universe contains many light scalar fields?\",\n",
    "\"Why is the mentioned gamma-ray binary system considered particularly interesting?\",\n",
    "\"What key assumption about turbulence underlies the basis of the present work on local equilibrium patches?\",\n",
    "\"How does the mass-ratio distribution of stellar companions to hot Jupiter systems compare to that of field star binaries?\",\n",
    "\"Which numerical model is employed for microswimmers, and how are the swimmers represented in this model?\",\n",
    "\"Why is the magnetic moment considered a fundamental property of the nucleon?\",\n",
    "\"Under what restriction does the algebra of constraints remain closed, avoiding inconsistency in quantum geometrodynamics and quantum field theory?\",\n",
    "\"What do clusters of galaxies trace in the context of large-scale cosmic structure?\",\n",
    "\"Why are radiative decays of heavy quarkonia useful for studying color-singlet two-gluon systems?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"QUESTION: {q}\")\n",
    "    answer, sources = rag_answer(q, use_multi_query=True)\n",
    "    print(f\"\\nANSWER: {answer}\")\n",
    "    print(f\"\\nSOURCES: {', '.join(set(sources))}\")\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-scientific-qa (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
